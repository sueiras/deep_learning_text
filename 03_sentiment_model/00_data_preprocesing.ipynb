{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "    - Download data to the server\n",
    "    - Convert text to sequences.\n",
    "    - Configure sequences for a RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to the server\n",
    "\n",
    "### Command line in the server\n",
    "    Path to data:\n",
    "        cd /home/ubuntu/data/training/text/sentiment\n",
    "    Download dataset: \n",
    "        wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    Uncompress it:\n",
    "        tar -zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert text to sequences\n",
    "    - List of all text files\n",
    "    - Read files into python\n",
    "    - Tokenize\n",
    "    - Create dictionaries to recode\n",
    "    - Recode tokens into ids and create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports and paths\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_path='../data/aclImdb/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "#Test\n",
    "#print(gen_find(\"*.txt\", data_path+'train/pos/').next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.', 'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****']\n"
     ]
    }
   ],
   "source": [
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r', encoding='utf-8') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences        \n",
    "\n",
    "#Test\n",
    "print(read_sentences(data_path+'train/pos/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", 'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.']\n"
     ]
    }
   ],
   "source": [
    "print(read_sentences(data_path+'train/neg/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "[['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer', '.', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'Watch', 'for', 'Alan', '``', 'The', 'Skipper', \"''\", 'Hale', 'jr.', 'as', 'a', 'police', 'Sgt', '.'], ['Bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by', 'Cristina', 'Raines', '(', 'later', 'of', 'TV', \"'s\", '``', 'Flamingo', 'Road', \"''\", ')', 'as', 'a', 'pretty', 'but', 'somewhat', 'unstable', 'model', 'with', 'a', 'gummy', 'smile', 'who', 'is', 'slated', 'to', 'pay', 'for', 'her', 'attempted', 'suicides', 'by', 'guarding', 'the', 'Gateway', 'to', 'Hell', '!', 'The', 'scenes', 'with', 'Raines', 'modeling', 'are', 'very', 'well', 'captured', ',', 'the', 'mood', 'music', 'is', 'perfect', ',', 'Deborah', 'Raffin', 'is', 'charming', 'as', 'Cristina', \"'s\", 'pal', ',', 'but', 'when', 'Raines', 'moves', 'into', 'a', 'creepy', 'Brooklyn', 'Heights', 'brownstone', '(', 'inhabited', 'by', 'a', 'blind', 'priest', 'on', 'the', 'top', 'floor', ')', ',', 'things', 'really', 'start', 'cooking', '.', 'The', 'neighbors', ',', 'including', 'a', 'fantastically', 'wicked', 'Burgess', 'Meredith', 'and', 'kinky', 'couple', 'Sylvia', 'Miles', '&', 'Beverly', \"D'Angelo\", ',', 'are', 'a', 'diabolical', 'lot', ',', 'and', 'Eli', 'Wallach', 'is', 'great', 'fun', 'as', 'a', 'wily', 'police', 'detective', '.', 'The', 'movie', 'is', 'nearly', 'a', 'cross-pollination', 'of', '``', 'Rosemary', \"'s\", 'Baby', \"''\", 'and', '``', 'The', 'Exorcist', \"''\", '--', 'but', 'what', 'a', 'combination', '!', 'Based', 'on', 'the', 'best-seller', 'by', 'Jeffrey', 'Konvitz', ',', '``', 'The', 'Sentinel', \"''\", 'is', 'entertainingly', 'spooky', ',', 'full', 'of', 'shocks', 'brought', 'off', 'well', 'by', 'director', 'Michael', 'Winner', ',', 'who', 'mounts', 'a', 'thoughtfully', 'downbeat', 'ending', 'with', 'skill', '.', '***1/2', 'from', '****']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence)]\n",
    "    print('Done!')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(read_sentences(data_path+'train/pos/')[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary..\n",
      "7056532  total words  134957  unique words\n",
      "2 289300\n"
     ]
    }
   ],
   "source": [
    "#create the dictionary to conver words to numbers. Order it with most frequent words first\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values()) # List of frequencies\n",
    "    keys = list(wordcount) #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "print(worddict['the'], wordcount['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333, 6, 25, 17, 225, 85, 1203, 68, 300, 35, 6, 189, 7, 917, 4853, 3630, 24, 19, 1572, 4, 4289, 6, 25, 141, 902, 16298, 9, 183, 182, 41, 7579, 16907, 9, 6, 157, 21646, 4, 21, 40749, 123, 9, 46, 1634, 3210, 4, 1231, 24, 1545, 32, 21, 23991, 31, 9489, 44509, 22, 6, 679, 7480, 4] 1\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "print(X_train_full[0], y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n",
      "[4142, 33, 46, 772, 80, 3, 320, 13655, 299, 2, 1654, 7, 46, 338, 1249, 3, 625, 631, 5, 531, 81, 2026, 5, 75, 20, 6298, 9222, 23, 52, 1908, 4, 137, 3791, 8, 49716, 23, 52, 852, 474, 50, 6, 63, 339, 8, 98, 271, 49, 16, 45, 3, 29, 73, 52, 34981, 20, 2652, 14, 1, 3, 75, 96, 36, 604, 2, 757, 23, 52, 852, 3, 5, 20, 911, 8, 864, 171, 377, 75, 96, 98, 108221, 4, 7836, 49, 2, 338, 32442, 4, 415, 2228, 14, 6, 316, 187, 75, 96, 2603, 58, 3, 75, 569, 6, 1234, 97, 2, 4420, 23, 6, 3708, 4907, 4, 32, 15, 802, 1643, 166, 14, 172, 4718, 15238, 3, 29, 194, 16642, 14, 87, 4, 15, 20, 4718, 559, 4, 31, 12, 13, 10, 11, 12, 13, 10, 11, 7056, 45, 779, 3189, 1967, 5, 75, 20, 1057, 14, 6, 1013, 10899, 4, 565, 73, 16, 613, 50, 75, 76, 4087, 5, 7972, 23268, 6, 1550, 3, 75, 234, 52, 3708, 4907, 98, 3837, 5, 351, 4, 142, 6, 3515, 380, 75, 858, 8, 1926, 49, 2, 781, 1550, 5, 373, 8, 2384, 104, 3, 23, 85, 215, 7, 769, 4, 121482, 52, 144, 20, 14, 2605, 4, 12, 13, 10, 11, 12, 13, 10, 11, 896, 9, 6, 272, 47, 7034, 9756, 3, 19787, 12709, 3, 21207, 52, 144, 8, 672, 198, 4, 5867, 7112, 9, 334, 3, 5, 36, 57, 8, 192, 43, 125, 75, 56, 6, 334, 581, 4, 61, 9, 46, 3413, 80, 4, 12, 13, 10, 11, 12, 13, 10, 11, 31, 335, 35, 4643, 17, 2294, 9, 2, 42, 2462, 17, 144, 228, 919, 4, 31]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "print(X_test_full[0])\n",
    "print(y_test_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure sequences for a RNN model\n",
    "    - Remove words with low frequency\n",
    "    - Truncate / complete sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length:  208.0\n"
     ]
    }
   ],
   "source": [
    "#Median length of sentences\n",
    "print('Median length: ', np.median([len(x) for x in X_test_full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 9, 6, 1572, 4, 214, 6, 1352, 4931, 391, 91, 2, 7546, 529, 20, 1020, 2106, 4, 6712, 23, 106, 460, 17, 1577, 87, 62, 6431, 4644, 127, 3, 108, 7468, 5, 353, 3216, 4, 51, 18, 239, 294, 4788, 8, 2, 246, 16, 18, 7542, 4, 335, 139, 166, 105, 602, 28, 42, 71, 521, 44, 2, 594, 7, 10695, 7, 6, 421, 14, 2, 3089, 27, 29, 100, 35, 4921, 8, 84, 16, 3, 6, 244, 50, 6, 549, 1754, 14, 633, 1563, 4, 21, 80, 14646, 107, 18113, 1379, 5, 1373, 62, 3770, 89, 36, 373, 5, 13532, 49, 800, 2, 42650, 3080, 7, 2, 562, 3, 22, 111, 1995, 23, 2, 3506, 7, 2, 101, 1750, 337, 543, 104, 3, 1503, 188, 48, 89, 30, 671, 104, 14, 74, 4, 51, 569, 87, 6, 184, 646, 8, 98, 58, 7, 2, 8659, 17, 17281, 87, 138, 342, 19, 0, 109, 704, 4]\n"
     ]
    }
   ],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 200)\n",
      "X_test shape: (25000, 200)\n",
      "[    2   338 32442     4   415  2228    14     6   316   187    75    96\n",
      "  2603    58     3    75   569     6  1234    97     2  4420    23     6\n",
      "  3708  4907     4    32    15   802  1643   166    14   172  4718 15238\n",
      "     3    29   194 16642    14    87     4    15    20  4718   559     4\n",
      "    31    12    13    10    11    12    13    10    11  7056    45   779\n",
      "  3189  1967     5    75    20  1057    14     6  1013 10899     4   565\n",
      "    73    16   613    50    75    76  4087     5  7972 23268     6  1550\n",
      "     3    75   234    52  3708  4907    98  3837     5   351     4   142\n",
      "     6  3515   380    75   858     8  1926    49     2   781  1550     5\n",
      "   373     8  2384   104     3    23    85   215     7   769     4     0\n",
      "    52   144    20    14  2605     4    12    13    10    11    12    13\n",
      "    10    11   896     9     6   272    47  7034  9756     3 19787 12709\n",
      "     3 21207    52   144     8   672   198     4  5867  7112     9   334\n",
      "     3     5    36    57     8   192    43   125    75    56     6   334\n",
      "   581     4    61     9    46  3413    80     4    12    13    10    11\n",
      "    12    13    10    11    31   335    35  4643    17  2294     9     2\n",
      "    42  2462    17   144   228   919     4    31]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras import preprocessing\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export train and test data\n",
    "np.save(data_path + 'X_train', X_train)\n",
    "np.save(data_path + 'y_train', y_train)\n",
    "np.save(data_path + 'X_test',  X_test)\n",
    "np.save(data_path + 'y_test',  y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export worddict\n",
    "import pickle\n",
    "\n",
    "with open(data_path + 'worddict.pickle', 'wb') as pfile:\n",
    "    pickle.dump(worddict, pfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
