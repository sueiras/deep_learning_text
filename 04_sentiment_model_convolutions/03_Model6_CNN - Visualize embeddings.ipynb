{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment model with CNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "data_path='../data/aclImdb/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n",
      "Building dictionary..\n",
      "7056532  total words  134957  unique words\n",
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n",
      "Preprocess done!\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "# Generator of list of files in a folder and subfolders\n",
    "def gen_find(filepattern, toppath):\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "            \n",
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r', encoding='utf-8') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences       \n",
    "\n",
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence)]\n",
    "    print('Done!')\n",
    "    return tokens\n",
    "\n",
    "def build_dict(sentences):\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values()) # List of frequencies\n",
    "    keys = list(wordcount) #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs\n",
    "#Data extraction\n",
    "\n",
    "#Extract training sentences\n",
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n",
    "\n",
    "#Build train dictionary\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "#Generate train data\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_trn = np.array([[1.,0.]]*len(train_x_pos) + [[0.,1.]]*len(train_x_neg), dtype=np.float32)\n",
    "\n",
    "\n",
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_tst = np.array([[1.,0.]]*len(test_x_pos) + [[0.,1.]]*len(test_x_neg), dtype=np.float32)\n",
    "\n",
    "\n",
    "print('Preprocess done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "max_len = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "\n",
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_trn = shuffle(X_train, y_trn, random_state=0)\n",
    "\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_trn = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_tst = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_trn.shape)\n",
    "print('X_test shape:', X_tst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "sequence_length = 100\n",
    "vocab_size = 5000\n",
    "\n",
    "embedding_size = 128\n",
    "num_filters = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "LOG_DIR = '/tmp/tensorboard/sentiment_cnn_2'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an interactive session\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.int32, shape=[None, 2], name=\"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "# Embedding layer\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    W_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W_embedding\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(W_embedding, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    \n",
    "   \n",
    "# Create a convolution + maxpool layer for each filter size\n",
    "def conv_layer(x, size_x=2, size_y=2, input_channels=1, output_channels=32):\n",
    "    W_conv = tf.Variable(tf.truncated_normal([size_x, size_y, input_channels, output_channels], stddev=0.1), name='W')\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[output_channels]), name='b')\n",
    "    conv_out = tf.nn.relu(tf.nn.conv2d(x, W_conv, strides=[1, 1, 1, 1], padding='VALID') + b_conv, name='conv')\n",
    "    pooled = tf.nn.max_pool(conv_out, ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "                            strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "    return pooled\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        pooled = conv_layer(embedded_chars_expanded, size_x=filter_size, size_y=embedding_size, input_channels=1, output_channels=num_filters)\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])    \n",
    "\n",
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.get_variable(\"W\", shape=[num_filters_total, 2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[2]), name=\"b\")\n",
    "    \n",
    "    l2_loss = tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "    \n",
    "# Calculate Mean cross-entropy loss\n",
    "loss_factor = 0.1\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y), name='loss') + loss_factor * l2_loss\n",
    "    loss_summ = tf.summary.scalar(\"Loss\", loss) #TENSORBOARD\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(input_y, 1)), \"float\"), name=\"accuracy\")\n",
    "    acc_summ = tf.summary.scalar(\"Accuracy\", accuracy) #TENSORBOARD\n",
    "    \n",
    "#Optimizer\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "with tf.name_scope('summaries') as scope:\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "    test_writer  = tf.summary.FileWriter(LOG_DIR + '/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the embeddings in tensorboard\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "\n",
    "# Create the embedding tensorboard container\n",
    "embedding = config.embeddings.add()\n",
    "\n",
    "# Link to the embeddings tensor\n",
    "embedding.tensor_name = W_embedding.name\n",
    "\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = LOG_DIR + '/train/records.tsv'\n",
    "\n",
    "# Saves a configuration file that TensorBoard will read during startup.\n",
    "projector.visualize_embeddings(train_writer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an embeddings metadata file to visualize the words\n",
    "\n",
    "# Inverse dictionary\n",
    "idx2w = {v: k for k, v in worddict.items()}\n",
    "\n",
    "with open(LOG_DIR + '/train/records.tsv', \"w\") as record_file:\n",
    "    record_file.write(\"UNK\\n\")\n",
    "    for item in list(idx2w.items())[:4999]:\n",
    "        record_file.write(str(item[1].encode('utf-8'))+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for inputs and targets.\n",
    "    \"\"\"\n",
    "    data_size = len(X)\n",
    "    # Shuffle the data at each epoch\n",
    "    shuffle_indices = np.random.permutation(np.arange(data_size))    \n",
    "    shuffled_X = X[shuffle_indices]\n",
    "    shuffled_y = y[shuffle_indices]\n",
    "        \n",
    "    num_batches = int((data_size-1)/batch_size) + 1\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        yield shuffled_X[start_index:end_index], shuffled_y[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e- LssTrn - AccTrn - LssTst - AccTst\n",
      "0 0.80719 0.635029 0.66074 0.719746\n",
      "1 0.616243 0.754313 0.652742 0.685371\n",
      "2 0.590014 0.769069 0.610722 0.735364\n",
      "3 0.54597 0.784192 0.553677 0.769954\n",
      "4 0.489181 0.821771 0.515197 0.783865\n",
      "5 0.436613 0.843455 0.474661 0.806035\n",
      "6 0.388939 0.863736 0.444235 0.817674\n",
      "7 0.34535 0.88137 0.431892 0.813672\n",
      "8 0.302616 0.897624 0.396345 0.834686\n",
      "9 0.260536 0.914955 0.387097 0.838512\n",
      "10 0.223954 0.9318 0.378002 0.84438\n",
      "11 0.190894 0.946158 0.378731 0.844292\n",
      "12 0.162107 0.959279 0.39217 0.843407\n",
      "13 0.135803 0.970464 0.399294 0.842379\n",
      "14 0.117362 0.978069 0.405588 0.84316\n",
      "15 0.0993539 0.987883 0.422704 0.841733\n",
      "16 0.0867738 0.992339 0.434518 0.840513\n",
      "17 0.0765028 0.995934 0.444312 0.840203\n",
      "18 0.0701116 0.996995 0.462289 0.839198\n",
      "19 0.0644432 0.998238 0.464926 0.837699\n"
     ]
    }
   ],
   "source": [
    "#Inicialization.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train proccess\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "step_trn = 0\n",
    "step_tst = 0\n",
    "print('e- LssTrn - AccTrn - LssTst - AccTst' )\n",
    "for epoch in range(num_epochs):\n",
    "    loss_trn = []\n",
    "    acc_trn = []\n",
    "    loss_tst = []\n",
    "    acc_tst = []\n",
    "    for x_batch, y_batch in batch_iter(X_trn, y_trn, batch_size):\n",
    "        train_step.run(feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 0.5})\n",
    "        loss_step, acc_step = sess.run([loss, accuracy], \n",
    "                                       feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1})\n",
    "        loss_trn += [loss_step]\n",
    "        acc_trn += [acc_step]\n",
    "        \n",
    "        # Summary over the last batch of the epoch\n",
    "        step_trn += 1\n",
    "        if step_trn % 10 ==0 :\n",
    "            summary_str = merged.eval(feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1})\n",
    "            train_writer.add_summary(summary_str, step_trn) #TENSORBOARD\n",
    "        \n",
    "    \n",
    "    for x_batch_test, y_batch_test in batch_iter(X_tst, y_tst, batch_size):\n",
    "        loss_step, acc_step = sess.run([loss, accuracy], \n",
    "                                       feed_dict={input_x: x_batch_test, input_y: y_batch_test, dropout_keep_prob: 1})\n",
    "        loss_tst += [loss_step]\n",
    "        acc_tst += [acc_step]\n",
    "        \n",
    "        # Summary over the last batch of the epoch\n",
    "        step_tst += 1 \n",
    "        if step_tst % 10 ==0 :\n",
    "            summary_str = merged.eval(feed_dict={input_x: x_batch_test, input_y: y_batch_test, dropout_keep_prob: 1})\n",
    "            test_writer.add_summary(summary_str, step_tst) #TENSORBOARD\n",
    "\n",
    "    print(epoch, np.mean(loss_trn), np.mean(acc_trn), np.mean(loss_tst), np.mean(acc_tst))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tensorboard/sentiment_cnn_2/train/model.ckpt-0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save checkpoint to visualize embeddings\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, LOG_DIR + '/train/model.ckpt', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
